package com.yahoo.ycsb.db;

import com.google.cloud.storage.*;
import com.yahoo.ycsb.ByteIterator;
import com.yahoo.ycsb.DB;
import com.yahoo.ycsb.DBException;
import com.yahoo.ycsb.Status;

import java.io.ByteArrayInputStream;
import java.io.InputStream;
import java.util.*;
import java.util.concurrent.atomic.AtomicInteger;

/**
 * Google Cloud Storage client for YCSB framework.
 *
 * Properties to set:
 *
 * The parameter table is the name of the Bucket where to upload the files.
 * This must be created before to start the benchmark
 * The size of the file to upload is determined by two parameters:
 * - fieldcount this is the number of fields of a record in YCSB
 * - fieldlength this is the size in bytes of a single field in the record
 * together these two parameters define the size of the file to upload,
 * the size in bytes is given by the fieldlength multiplied by the fieldcount.
 * The name of the file is determined by the parameter key.
 *This key is automatically generated by YCSB.
 *
 */
public class GoogleCloudStorageClient extends DB {

  private Storage storage;
  private static final AtomicInteger INIT_COUNT = new AtomicInteger(0);

  /**
   * Cleanup any state for this storage.
   * Called once per S3 instance;
   */
  @Override
  public void cleanup() throws DBException {
  }

  /**
   * Delete a file from S3 Storage.
   *
   * @param bucket
   *            The name of the bucket
   * @param key
   * The record key of the file to delete.
   * @return OK on success, otherwise ERROR. See the
   * {@link DB} class's description for a discussion of error codes.
   */
  @Override
  public Status delete(String bucket, String key) {
    return null;
  }

  /**
   * Initialize any state for the storage.
   * Called once per S3 instance; If the client is not null it is re-used.
   */
  @Override
  public void init() throws DBException {
    final int count = INIT_COUNT.incrementAndGet();
    synchronized (GoogleCloudStorageClient.class){
      Properties propsCL = getProperties();
      int recordcount = Integer.parseInt(
          propsCL.getProperty("recordcount"));
      int operationcount = Integer.parseInt(
          propsCL.getProperty("operationcount"));
      int numberOfOperations = 0;
      if (recordcount > 0){
        if (recordcount > operationcount){
          numberOfOperations = recordcount;
        } else {
          numberOfOperations = operationcount;
        }
      } else {
        numberOfOperations = operationcount;
      }
      if (count <= numberOfOperations) {
        try {
          storage = StorageOptions.getDefaultInstance().getService();
          System.out.println("Connection successfully initialized");
        } catch (Exception e){
          System.err.println("Could not connect to Google Cloud Storage: "+ e.toString());
          e.printStackTrace();
          throw new DBException(e);
        }
      } else {
        System.err.println(
            "The number of threads must be less or equal than the operations");
        throw new DBException(new Error(
            "The number of threads must be less or equal than the operations"));
      }
    }
  }

  /**
   * Create a new File in the Bucket. Any field/value pairs in the specified
   * values HashMap will be written into the file with the specified record
   * key.
   *
   * @param bucket
   *            The name of the bucket
   * @param key
   *      The record key of the file to insert.
   * @param values
   *            A HashMap of field/value pairs to insert in the file.
   *            Only the content of the first field is written to a byteArray
   *            multiplied by the number of field. In this way the size
   *            of the file to upload is determined by the fieldlength
   *            and fieldcount parameters.
   * @return OK on success, ERROR otherwise. See the
   *         {@link DB} class's description for a discussion of error codes.
   */
  @Override
  public Status insert(String bucket, String key,
                       Map<String, ByteIterator> values) {
    return writeToStorage(bucket, key, values);
  }

  /**
   * Read a file from the Bucket. Each field/value pair from the result
   * will be stored in a HashMap.
   *
   * @param bucket
   *            The name of the bucket
   * @param key
   *            The record key of the file to read.
   * @param fields
   *            The list of fields to read, or null for all of them,
   *            it is null by default
   * @param result
   *          A HashMap of field/value pairs for the result
   * @return OK on success, ERROR otherwise.
   */
  @Override
  public Status read(String bucket, String key, Set<String> fields,
                     Map<String, ByteIterator> result) {
    return Status.OK;
  }

  /**
   * Update a file in the database. Any field/value pairs in the specified
   * values HashMap will be written into the file with the specified file
   * key, overwriting any existing values with the same field name.
   *
   * @param bucket
   *            The name of the bucket
   * @param key
   *            The file key of the file to write.
   * @param values
   *            A HashMap of field/value pairs to update in the record
   * @return OK on success, ERORR otherwise.
   */
  @Override
  public Status update(String bucket, String key,
                       Map<String, ByteIterator> values) {
    return writeToStorage(bucket, key, values);
  }

  /**
   * Perform a range scan for a set of files in the bucket. Each
   * field/value pair from the result will be stored in a HashMap.
   *
   * @param bucket
   *            The name of the bucket
   * @param startkey
   *            The file key of the first file to read.
   * @param recordcount
   *            The number of files to read
   * @param fields
   *            The list of fields to read, or null for all of them
   * @param result
   *            A Vector of HashMaps, where each HashMap is a set field/value
   *            pairs for one file
   * @return OK on success, ERROR otherwise.
   */
  @Override
  public Status scan(String bucket, String startkey, int recordcount,
                     Set<String> fields, Vector<HashMap<String, ByteIterator>> result) {
    return Status.OK;
  }

  /**
   * Upload a new object to S3 or update an object on S3.
   *
   * @param bucket
   *            The name of the bucket
   * @param key
   *            The file key of the object to upload/update.
   * @param values
   *            The data to be written on the object
   *
   */
  protected Status writeToStorage(String bucket, String key,
                                  Map<String, ByteIterator> values) {
    int totalSize = 0;
    int fieldCount = values.size(); //number of fields to concatenate
    // getting the first field in the values
    Object keyToSearch = values.keySet().toArray()[0];
    // getting the content of just one field
    byte[] sourceArray = values.get(keyToSearch).toArray();
    int sizeArray = sourceArray.length; //size of each array
    totalSize = sizeArray*fieldCount;
    byte[] destinationArray = new byte[totalSize];
    int offset = 0;
    for (int i = 0; i < fieldCount; i++) {
      System.arraycopy(sourceArray, 0, destinationArray, offset, sizeArray);
      offset += sizeArray;
    }
    try (InputStream input = new ByteArrayInputStream(destinationArray)) {
      BlobId blobId = BlobId.of(bucket, key);
      BlobInfo blobInfo = BlobInfo.newBuilder(blobId).setContentType("text/plain").build();
      Blob blob = storage.create(blobInfo, input);
    } catch (Exception e) {
      System.err.println("Error in the creation of the stream :"+e.toString());
      e.printStackTrace();
      return Status.ERROR;
    }

    return Status.OK;
  }
}
